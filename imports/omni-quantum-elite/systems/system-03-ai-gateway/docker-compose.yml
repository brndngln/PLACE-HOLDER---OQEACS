###############################################################################
# System 3 — AI Gateway (LiteLLM)
# Unified LLM proxy — routes to OpenAI, Anthropic, Ollama, etc.
# with load balancing, rate limiting, and spend tracking.
###############################################################################
services:
  omni-litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: omni-litellm
    restart: unless-stopped
    ports:
      - "4000:4000"
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@omni-postgres:5432/litellm
      STORE_MODEL_IN_DB: "True"
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-omni-litellm-master}
      VAULT_ADDR: http://omni-vault:8200
      VAULT_TOKEN: ${VAULT_TOKEN:-}
      OLLAMA_API_BASE: http://omni-ollama:11434
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
    volumes:
      - ./config/litellm-config.yaml:/app/config.yaml:ro
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    depends_on:
      omni-postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-fs", "http://localhost:4000/health"]
      interval: 15s
      timeout: 5s
      retries: 5
    networks:
      - omni-quantum-network
    labels:
      omni.system: "3"
      omni.name: "AI Gateway"
      omni.tier: "critical"

networks:
  omni-quantum-network:
    external: true
